# -*- coding: utf-8 -*-
"""Irish Wildlife Statistics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XpCqu_rl1qEOV6phDaon5JyH-tH_2O1P
"""

# Import necessary libraries and suppress warnings
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Ensure compatibility in environments like Kaggle
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

sns.set(style='whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)

from google.colab import files
up=files.upload()

# Load the dataset
df = pd.read_csv('irish_animals.csv', encoding='ascii')

# Display the first few rows to get a glimpse of the data
print('First few rows of the dataset:')
print(df.head())

# Display basic information
print('\nDataset Info:')
print(df.info())

# Summary statistics for numeric columns
print('\nSummary Statistics:')
print(df.describe())

# Checking for missing values
print('Missing values in each column:')
print(df.isnull().sum())

# If there were any issues with data types, they would be handled here
# For example, if numeric columns were read as object types, then conversion would be necessary
# In our dataset, the column types are as expected.

# Separate numeric columns for further analysis
numeric_df = df.select_dtypes(include=[np.number])

# Correlation Heatmap (only if there are 4 or more numeric columns)
if numeric_df.shape[1] >= 4:
    plt.figure(figsize=(10, 6))
    corr = numeric_df.corr()
    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
    plt.title('Correlation Heatmap of Numeric Features')
    plt.tight_layout()
    plt.show()
else:
    print('Not enough numeric values for a correlation heatmap.')

# Pair Plot for numeric columns - a glimpse into pairwise relationships
#sns.pairplot(numeric_df)
#plt.suptitle('Pair Plot of Numeric Features', y=1.02)
#plt.show()

# Histograms for each numeric column
numeric_columns = numeric_df.columns

for col in numeric_columns:
    plt.figure()
    sns.histplot(numeric_df[col], kde=True, bins=20)
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()

# Pie Chart / Count Plot for categorical 'Name' column
plt.figure(figsize=(12, 6))
sns.countplot(y='Name', data=df, order=df['Name'].value_counts().index)
plt.title('Count Plot for Animal Names')
plt.xlabel('Count')
plt.ylabel('Animal Name')
plt.tight_layout()
plt.show()

# Create new features: average size, weight, and lifespan
df['avg_size'] = (df['Size (min cm)'] + df['Size (max cm)']) / 2
df['avg_weight'] = (df['Weight (min kg)'] + df['Weight (max kg)']) / 2
df['avg_lifespan'] = (df['Lifespan (min years)'] + df['Lifespan (max years)']) / 2

# Define features and target variable
features = ['avg_size', 'avg_weight', 'avg_lifespan']
target = 'Population'

# Split the data into training and testing sets
X = df[features]
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a simple Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Predict on the test set
y_pred_lr = lr_model.predict(X_test)

# Calculate evaluation metrics for Linear Regression
r2_lr = r2_score(y_test, y_pred_lr)
mse_lr = mean_squared_error(y_test, y_pred_lr)
print('Linear Regression Performance:')
print(f'R-squared: {r2_lr:.4f}')
print(f'Mean Squared Error: {mse_lr:.4f}')

# For a non-linear alternative, let us also try a Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Calculate evaluation metrics for Random Forest Model
r2_rf = r2_score(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
print('\nRandom Forest Regression Performance:')
print(f'R-squared: {r2_rf:.4f}')
print(f'Mean Squared Error: {mse_rf:.4f}')

# Optional: Plot predicted vs. actual populations for a visual check
plt.figure()
plt.scatter(y_test, y_pred_rf, color='teal', edgecolor='k', alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], ls='--', color='red')
plt.xlabel('Actual Population')
plt.ylabel('Predicted Population')
plt.title('Random Forest: Actual vs. Predicted Population')
plt.tight_layout()
plt.show()

